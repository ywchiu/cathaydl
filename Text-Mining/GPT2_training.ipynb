{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 安裝 Transformers"
      ],
      "metadata": {
        "id": "7zpoimZdNLYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iITQf0TJVEyu",
        "outputId": "a11c765d-8d65-4d97-def6-5d3e04a3ee21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 運用GPT2 產生預測結果"
      ],
      "metadata": {
        "id": "VcLveV_DNSiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Encode a text inputs\n",
        "text = \"What is the fastest car in the\"\n",
        "indexed_tokens = tokenizer.encode(text)\n",
        "\n",
        "# Convert indexed tokens in a PyTorch tensor\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "tokens_tensor = tokens_tensor.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]\n",
        "\n",
        "# Get the predicted next sub-word\n",
        "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
        "\n",
        "# Print the predicted word\n",
        "print(predicted_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je_k_XTnT5g2",
        "outputId": "d0585622-136e-4c01-b4c8-431d9cc0fa9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the fastest car in the world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 產生更長的輸出"
      ],
      "metadata": {
        "id": "0lKz00uQNYy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "generator(\"Hello,\", max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmcVbZFrWWyO",
        "outputId": "04543b2b-670d-40cf-ec2f-3f801ae3f461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Hello, but what I'm really curious about is who I actually met on my wedding night in Las Vegas. The story doesn't end there: I\"},\n",
              " {'generated_text': \"Hello, this is not the best and I don't like it. I hope you didn't understand this when you did these pictures. For the record\"},\n",
              " {'generated_text': \"Hello, if you've chosen, if you've given, if you know, it's been a privilege to have it be so for this last year\"},\n",
              " {'generated_text': \"Hello, you can do that, too!\\n\\nWhy, you said the answer was simple, and so you did it, and so I'm\"},\n",
              " {'generated_text': \"Hello, I've been watching a TV series called G.I. Joe and watched it for quite a while. It was one of those sitcoms\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 產生 Embedding"
      ],
      "metadata": {
        "id": "sSsS-KRBNe8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input sentence\n",
        "sentence = \"hello, world\"\n",
        "\n",
        "# Encode input sentence using the tokenizer\n",
        "input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
        "\n",
        "# Convert input tensor to PyTorch\n",
        "input_ids = input_ids.to(torch.int64).to('cuda')\n",
        "\n",
        "# Pass input tensor through the model to get embeddings\n",
        "with torch.no_grad():\n",
        "    embeddings = model(input_ids)[0]"
      ],
      "metadata": {
        "id": "kDx7dw8CWjzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QECsMdAUYOCH",
        "outputId": "a3561953-7a51-44db-b215-7417bcc608af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ -37.0708,  -36.4855,  -40.3520,  ...,  -46.5168,  -45.4142,\n",
              "           -37.9090],\n",
              "         [-108.6662, -109.3310, -110.0967,  ..., -114.7922, -112.9426,\n",
              "          -106.3587],\n",
              "         [ -72.9768,  -73.6322,  -75.5144,  ...,  -86.3169,  -81.2544,\n",
              "           -74.9500]]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 微調(Fine-Tuning) GPT2"
      ],
      "metadata": {
        "id": "gLqzDyn-NkYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://github.com/ywchiu/sns_mining/raw/main/data/lord_of_rings.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFHZ1Q7-aP49",
        "outputId": "08d7705f-f1fc-4379-9b4c-44d91b8b13e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-21 15:24:56--  https://github.com/ywchiu/sns_mining/raw/main/data/lord_of_rings.txt\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ywchiu/sns_mining/main/data/lord_of_rings.txt [following]\n",
            "--2023-04-21 15:24:56--  https://raw.githubusercontent.com/ywchiu/sns_mining/main/data/lord_of_rings.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1021631 (998K) [text/plain]\n",
            "Saving to: ‘lord_of_rings.txt.1’\n",
            "\n",
            "lord_of_rings.txt.1 100%[===================>] 997.69K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-04-21 15:24:56 (55.6 MB/s) - ‘lord_of_rings.txt.1’ saved [1021631/1021631]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "HtuM5ckwZZ_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        "\n",
        "\n",
        "def load_dataset(file_path, tokenizer):\n",
        "    dataset = LineByLineTextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=128,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "\n",
        "file_path = \"load_of_rings.txt\"\n",
        "train_dataset = load_dataset(file_path, tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpmVERFkYVEI",
        "outputId": "30bb77b4-1044-4eb8-a823-70bf3f024f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "0ZqNCSLsY7aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "3cGxzcRfQKpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "kGsK1HiQQQHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CYLWE1iPa2j6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}